---
layout: about
title: about
permalink: /
subtitle: Data Science | Neuroscience | AI/ML</a>.

profile:
  align: right
  image: prof_pic.png
  image_circular: true # crops the image to make it circular
  more_info: >
    <p>Chief Data Analytics Office</p>
    <p>Vanguard</p>
    <p>Malvern, PA</p>

news: false # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page
---

Travis Meyer, is a Senior Data Scientist in Customer Experience at Vanguard. He uses his background as a Neuroscientist at the [University of Pennsylvania](https://www.med.upenn.edu/neuroscience/), and Deep Learning at [Carnegie Mellon University](https://www.cmu.edu/) to develop artifical intelligence tools that process and simplify large amounts of data. 

### research focus

Travis' early work focused on learning how neural networks in the visual system worked by studying learning, memory and complex visual functions:

- [Working Memory in Prefrontal Cortex Neural Circuits](https://www.researchgate.net/publication/245628571_Effects_of_training_to_perform_a_working_memory_Task_on_regular_spiking_and_fast_spiking_neurons_in_the_lateral_prefrontal_cortex) and [Learning in Prefrontal Cortex](https://www.researchgate.net/publication/51081929_Stimulus_Selectivity_in_Dorsal_and_Ventral_Prefrontal_Cortex_after_Training_in_Working_Memory_Tasks) as part of his PhD Thesis.
- To acomplish this he developed [a new software IDE](https://www.researchgate.net/publication/8078008_A_software_solution_for_the_control_of_visual_behavioral_experimentation) to control and coordinate the experimental hardware and store the data securely.
- He surprisingly found that the brain automatically cached information unconsciously [publication]https://www.researchgate.net/publication/6114369_Persistent_Discharges_in_the_Prefrontal_Cortex_of_Monkeys_Naive_to_Working_Memory_Tasks)
- Applying [singular learning theory](https://edmundlth.github.io/posts/overview-of-singular-learning-theory/) to examine phase transitions in algorithmic tasks.
- Mechanistically interpreting [prior-fitted tabular transformers](https://arxiv.org/abs/2207.01848).
- Creating sparse boolean circuits (inspired by [computation in superposition](https://arxiv.org/abs/2408.05451)) as testbeds and benchmarks for interpretability methods.
 
If you find any of these topics interesting, please reach out.

As part of the [AI Safety Initiative Amsterdam](https://aisafetyamsterdam.com/), I'm actively involved in promoting AI safety research and awareness. We organize events, facilitate reading groups, and foster discussions on crucial AI safety topics.

I'm also passionate about nurturing the next generation of AI safety researchers. I've been involved in teaching courses and supervising numerous Master's students on projects ranging from detecting bias, eliciting truth in LLMs, to interpretability in medical AI applications.

### beyond research

When I'm not diving into the intricacies of neural networks, you might find me:

- Practicing yoga or meditation to maintain balance.
- Reading science-fiction novels (recently discovered Vernor Vinge's work for a plausible treatment of AI singularity).
- Playing with my two chihuahuas, Cicchetti and Pancetta. 
- Exploring Amsterdam's culinary scene (always on the lookout for the best vegan spots!)
- Brushing up on my Mandarin or picking up Dutch phrases.
- Engaging in discussions about the future of AI and its implications for society.